{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Represetning Text With Contextual Embeddings\r\n",
    "\r\n",
    "Before we dive into buiding our own Question and Answering model the first step if to understand how State of the Art PyTorch SOTA models represent text with Contextual Embeddings. \r\n",
    "\r\n",
    "At this point you may be wondering what the term 'Contextual Embedding' even means. Don't worry by the end of this module that will be abundently clear. In order to understand better the concept lets first take a step back and look at problem of textual representation, some of the different approaches that have culminated in the current state of the art.\r\n",
    "\r\n",
    "At the in the third module we will look at sample PyTorch code using the HuggingFace transformers library that use contextual embeddings to make our Question and Answering model work.\r\n",
    "\r\n",
    "# What is Text Representation?\r\n",
    "\r\n",
    "If you are here you proably at some point learned how to read, write and process language. Computers represent textual characters as numbers that map to fonts on your screen using coding formats such as ASCII or UTF-8. \r\n",
    "\r\n",
    "![Ascii Code](images/ASCII.png)\r\n",
    "\r\n",
    "You and I can understand what the letters the fonts mapped to these codes **represent** and how each of their characters come together to form the words of this sentence. However computers by themselves do not have such an understanding. Therefore we need we need a mechanism to represent **text features** as features such as words and characters in order to train langugage models.\r\n",
    "\r\n",
    "# How do we Represent Text with Computers?\r\n",
    "\r\n",
    "There are many different approaches to represent textual features in a format that can be modeled with machine learning. We've already mentioned that contextual embeddings is the state of the art but before we explain how contextual embeddigns work lets take a brief tour of the more tradional and early neural approaches for representing text.\r\n",
    "\r\n",
    "## Bag of Words Text Representation\r\n",
    "\r\n",
    "Bag of Words or BoW vector representations are the most common used traditional vector representation. Each word or n-gram is linked to a vector index and marked as 0 or 1 depending on whether it occurs in a given document.\r\n",
    "\r\n",
    "![bow image here](images/bow.png) \r\n",
    "\r\n",
    "BoW representations are often used in methods of document classification where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. One challenge with bag of word representations is that they don’t encode any information with regards to the meaning of a given word.\r\n",
    "\r\n",
    "In BoW word occurrences are evenly weighted independently of how frequently or what context they occur. However in most NLP tasks some words are more relevant than others.\r\n",
    "\r\n",
    "## Term Frequency Inverse Document Frequency TF - IDF\r\n",
    "\r\n",
    "TF - IDF short for term frequency–inverse document frequency, is a variation of bag of words where instead of a binary 0 and 1 value being used to indicate the appearence of a ngram in a document a the TF-IDF value is used. The TF-IDF value is a numerical statstic that reflect how prominent a word or n-gram is to a document in a collection. The TF-IDF value increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently than others. \r\n",
    "\r\n",
    "![tfidf image here](images/tfidf.png)\r\n",
    "\r\n",
    "However even though TF-IDF representations provide frequency weight to different words they are unable to represent meaning or order. As the famous linguist J. R. Firth said in 1935, “The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously.”\r\n",
    "\r\n",
    "## Traditional Distributional Embeddings with Mutual Information\r\n",
    "\r\n",
    "Distributional Embeddings enable word vectors to encapsulate contextual context. Each embedding vector is represented based on the mutual information it has with other words in a given corpus. Mutual information can be represented as a global co-occurrence frequency or restricted to a given window either sequentially or based on dependency edges.\r\n",
    "\r\n",
    "![distributional matrix image here](images/dist_matrix.png)\r\n",
    "\r\n",
    "While these distributional methods were comparable to later neural embeddings they were difficult to implement, computationally expensive and not widely used in the industry. \r\n",
    "\r\n",
    "## PreTrained Embeddings Word2Vec and Varients\r\n",
    "\r\n",
    "As opposed to traditional distributional models neural embeddings such as Word to Vec are learned by training a neural langauge model to minimize a loss function for tasks that map to language understanding.  This process of training models on large collections of text to extract word representaions is called pre-training.  \r\n",
    "\r\n",
    "One of the first sucessful neural pretraining techniques for text representation was called Word2Vec. \r\n",
    "\r\n",
    "There are two main architectures that are used to produce a distributed representation of words:\r\n",
    "\r\n",
    " - Continuous bag-of-words (CBOW) — In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.\r\n",
    " - Continuous skip-gram - In the continuous skip-gram architecture, the model uses surrounding window of context words to predict the current word.\r\n",
    "\r\n",
    "CBOW is faster while skip-gram is slower but does a better job of representing infrequent words.\r\n",
    "\r\n",
    "![word2vec image](images/word2vec.png)\r\n",
    "\r\n",
    "Both CBOW and Skip-Grams are “predictive” embeddings, in that they only take local contexts into account. Word2Vec does not take advantage of global context.FastText, built on Word2Vec by learning vector representations for each word and the charachter n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to pre-training it enables word embeddings to encode sub-word information. \r\n",
    "\r\n",
    "Another method called GloVe by contrast leverages the same intuition behind the co-occurence matrix used by the traditional distributional embeddings above, but uses neural methods to decompose the co-occurrence matrix into more expressive and non linear word vectors.\r\n",
    "\r\n",
    "One key limitation of tradition pretrained embedding representaitons such as Word2Vec is the problem of word sense disambigioution. While pretrained embeddings can capture some of the meaning of words in context every possible meaning of a word is encoded into the same embedding. This can cause problems in downstream models since many words such as the word 'play' have different meanings depending on the context they are used in.\r\n",
    "\r\n",
    "For example word 'play' in the the sentence\r\n",
    "- I went to a [play] at the theature.\r\n",
    "Does not mean the same thing as the word 'play' in the sentence.\r\n",
    "- John wan't to [play] with his friends.\r\n",
    "\r\n",
    "The pretrained embeddings above represent both of these meanings of the word 'play' in the same embedding. Contextual embeddings ,methods were developed to address this challenge of disambigutation and contributed to the massive leap forward in natrual language processing applications. \r\n",
    "\r\n",
    "## Contextual Embeddings\r\n",
    "\r\n",
    "To address challenges of word sense disambigution a new method of pretraining models on large amounts of data and using the pre-trained models to generate contextual embeddings was spearheaded with the advent of models such as ULMFiT, ELMO and Later BERT.\r\n",
    "\r\n",
    "![elmo](images/elmo.png)\r\n",
    "\r\n",
    "ULMFit and ELMo were models that generates embedding for a word based on the context it appears thus generating slightly different embeddings for each of its occurrence and thus alowing a downstream model to better disambiguate between the correct sense of a given word such as 'play'. On in it’s release it enabled near instant state of the art results in many downstream tasks, including tasks such as co-reference were previously not as viable for practical usage in nlp.\r\n",
    "\r\n",
    "This spawned was coined as the ImageNet moment of NLP more recent transfomer based models such as BERT capitalize on the development of BERT using attention transformers instead of bi-directonal RNNs to encode context. If you are unfamiliar with terms such as Transformers and RNNs do not worry in the next module we will walk through the progression of NLP models culminiating in the advent of current state of the models in NLP with PyTorch. \r\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit74d6bbee72be4203a51c7d8693e15448"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}