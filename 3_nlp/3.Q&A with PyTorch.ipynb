{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\r\n",
    "\r\n",
    "In the previous two modules we discussed how to represent and model text using transformer models. In the following module we will combine all the topics we discussed to finetune a transformer model for Question and Answering using the hugging face library and PyTorch. The code in this example is based on the offical Hugging Face tutorial that can be found [here](https://huggingface.co/transformers/custom_datasets.html#qa-squad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is [Question Answering](https://en.wikipedia.org/wiki/Question_answering#:~:text=Question%20answering%20(QA)%20is%20a,humans%20in%20a%20natural%20language)?\r\n",
    "\r\n",
    "Question answering is concerned with building models that answer questions posed by humans. Question answering is an important NLP task that requires a model to be able to understand a human written question, map the question text to its own knowledge of the world, and generate a understandable natural language response. Due to it's complexity the task of Question Answering is often used to test the robustness of natrual langauge understanding models. \r\n",
    "\r\n",
    "While not a fully solved task, current state of the art transformer models preform well on when finetuned on baseline extractive question answering datasets such as SQUAD v2. Extractive QA involves answering a question about a passage by highlighting the segment of the passage that answers the question. These models can be used to help build more contextual bot applications for automating all sorts of important yet time consuming industry tasks from supporting primary care physicians, laywers, and customer support centers triage common questions to better indexing information.  \r\n",
    "\r\n",
    "In the following notebook we will walk through what we learned about contextual embedding and transformers and build are own Q&A model with PyTorch and HuggingFace Library.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a PreTrained DistilBERT Question Answering Model Using HuggingFace \r\n",
    "\r\n",
    "Model distillation is a neural model compression technique in which a small network (student) is taught by a larger trained neural network (teacher). [DistilBERT](https://medium.com/huggingface/distilbert-8cf3380435b5) is a small, fast, cheap and light Transformer model provided by HuggingFace trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\r\n",
    "\r\n",
    "In this section we will show how to load a pretrained DistilBert model for question and answering and use it to answer our own sample question about Microsoft given what we've learned about contextual embedding and transformer models.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must prepare our input context and question for our pretrained model. For our context I've taken the first to sentences from the Microsoft wikipedia page and then will ask a simple question about what does Microsoft manufacture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Microsoft Corporation is an American multinational technology company with headquarters in Redmond, Washington. It develops, manufactures, licenses, supports, and sells computer software, consumer electronics, personal computers, and related services.\"\r\n",
    "question = \"What does Microsoft manufacture?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to represent our input text and extract our contextual embeddings using the DistilBert tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\r\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased',return_token_type_ids = True)\r\n",
    "encoding = tokenizer.encode_plus(question, context)\r\n",
    "input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've encoded our inputs we need to load our pretrained distilBERT model and set it to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForQuestionAnswering\r\n",
    "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\r\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then pass our represented inputs to the model and get the predicted spans of our context that contain the answer to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_scores, end_scores = model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to extract the tokens ids from the context that our model was most confident contains the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ans_tokens = input_ids[torch.argmax(start_scores) : torch.argmax(end_scores)+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have these tokens ids we want to convert them to their tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_tokens = tokenizer.convert_ids_to_tokens(ans_tokens , skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then from the tokens we can finally extract the plain text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_tokens_to_string = tokenizer.convert_tokens_to_string(answer_tokens)\r\n",
    "\r\n",
    "print(\"Context: '\\n\", context)\r\n",
    "print(\"Question: '\\n\", question)\r\n",
    "print(\"Encoding: '\\n\", encoding)\r\n",
    "print(\"Model Answer : \\n\", answer_tokens_to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we see that our pretrained model correctly answers our question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tunning Our Model on a Custom Dataset\r\n",
    "\r\n",
    "Now that we have demonstrated that our pretrained question and answering model works. I will show how we can fine tune the model to on a custom dataset using PyTorch and Squad v2 based on the [HuggingFace documentation](https://huggingface.co/transformers/custom_datasets.html#qa-squad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must download our the custom Squad_V2 dataset while this dataset can be loaded using Hugging Face datasets library for the sake of showing you how to later train on your own data we will download and process all the train and test files directly.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir squad\r\n",
    "! wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json\r\n",
    "! wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define a helper function to load the squad data from json into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "def read_squad(path):\r\n",
    "    path = Path(path)\r\n",
    "    with open(path, 'rb') as f:\r\n",
    "        squad_dict = json.load(f)\r\n",
    "\r\n",
    "    contexts, questions, answers = [], [], []\r\n",
    "    for group in squad_dict['data']:\r\n",
    "        for passage in group['paragraphs']:\r\n",
    "            context = passage['context']\r\n",
    "            for qa in passage['qas']:\r\n",
    "                question = qa['question']\r\n",
    "                for answer in qa['answers']:\r\n",
    "                    contexts.append(context)\r\n",
    "                    questions.append(question)\r\n",
    "                    answers.append(answer)\r\n",
    "\r\n",
    "    return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squad and Squad_v2 are known for having some alignement mistakes in their training data hopefully this will not happen on your custom data but if it does it is good to look at the following alingment functions for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "\r\n",
    "def add_end_idx(answers, contexts):\r\n",
    "    \"\"\"\r\n",
    "    # sometimes squad answers are off by a character or two – fix this\r\n",
    "    \"\"\"\r\n",
    "    for answer, context in zip(answers, contexts):\r\n",
    "        gold_text = answer['text']\r\n",
    "        start_idx = answer['answer_start']\r\n",
    "        end_idx = start_idx + len(gold_text)\r\n",
    "\r\n",
    "        if context[start_idx:end_idx] == gold_text:\r\n",
    "            answer['answer_end'] = end_idx\r\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\r\n",
    "            answer['answer_start'] = start_idx - 1\r\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\r\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\r\n",
    "            answer['answer_start'] = start_idx - 2\r\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\r\n",
    "\r\n",
    "def add_token_positions(encodings, answers, tokenizer):\r\n",
    "    \"\"\"\r\n",
    "    helper function to keep track of token positions\r\n",
    "    \"\"\"\r\n",
    "    start_positions = []\r\n",
    "    end_positions = []\r\n",
    "    for i in range(len(answers)):\r\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\r\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\r\n",
    "        # if None, the answer passage has been truncated\r\n",
    "        if start_positions[-1] is None:\r\n",
    "            start_positions[-1] = tokenizer.model_max_length\r\n",
    "        if end_positions[-1] is None:\r\n",
    "            end_positions[-1] = tokenizer.model_max_length\r\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've defined our helper functions we can create our own Custom PyTorch Dataset Class for Training our Squad V2 data it takes as input the path to the squad json file and a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\r\n",
    "    def __init__(self, path, tokenizer):\r\n",
    "        contexts, questions, answers = read_squad(path) # process the json file\r\n",
    "        add_end_idx(answers, contexts) # align mistakes\r\n",
    "        self.encodings = tokenizer(contexts, questions, truncation=True, padding=True) # tokenize data\r\n",
    "        add_token_positions(self.encodings, answers, tokenizer) # add encoded token positions \r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this class to process our training and testing data using the tokenizer we used for representing our input data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset('squad/train-v2.0.json', tokenizer)\r\n",
    "val_dataset = SquadDataset('squad/dev-v2.0.json', tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset process we can easily fine tune the pretrained model we used before using a standard PyTorch training loop. Note due to the size of squad this code may take some time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from transformers import AdamW\r\n",
    "\r\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
    "\r\n",
    "model.to(device)\r\n",
    "model.train()\r\n",
    "\r\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\r\n",
    "\r\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\r\n",
    "\r\n",
    "for epoch in range(3):\r\n",
    "    print(\"Epoch #\", epoch)\r\n",
    "    for i, batch in enumerate(train_loader):\r\n",
    "        optim.zero_grad()\r\n",
    "        input_ids = batch['input_ids'].to(device)\r\n",
    "        attention_mask = batch['attention_mask'].to(device)\r\n",
    "        start_positions = batch['start_positions'].to(device)\r\n",
    "        end_positions = batch['end_positions'].to(device)\r\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\r\n",
    "        loss = outputs[0]\r\n",
    "        print(f'Step {i} - loss: {loss:.3}')\r\n",
    "        loss.backward()\r\n",
    "        optim.step()\r\n",
    "\r\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it we now have a fine tuned QA DistilBert Model that we trained on our own custom data. Now you should have all the tools you need to train your own Question Answering models as well as strong fundemental understanding of the concepts of Text Representation, Language Modeling and Neural Natrual Language Processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python38364bit74d6bbee72be4203a51c7d8693e15448"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}