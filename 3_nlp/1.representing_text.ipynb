{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Represetning Text With Contextual Embeddings\r\n",
    "\r\n",
    "Before we dive into buiding our own Question and Answering model the first step if to understand how State of the Art PyTorch SOTA models represent text with Contextual Embeddings. \r\n",
    "\r\n",
    "At this point you maybe wondering what the term 'Contextual Embedding' even means.  Don't worry by the end of the module that will be abundently clear but in order to understand better the concept lets first take a step back and look at problem of textual representation, some of the different approaches that have culminated in the current state of the art.\r\n",
    "\r\n",
    "At the end of this tour we will look at sample PyTorch code using the HuggingFace transformers library to generate contextual embeddings.\r\n",
    "\r\n",
    "# What is Text Representation\r\n",
    "\r\n",
    "If you are here you proably at some point learned how to read, write and process language. Computers represent textual characters as numbers that map to fonts on your screen using coding formats such as ASCII or UTF-8. You and I can understand what the letters these fonts **represent** and how they characters come together to form the words of this setence. However in order by themselves computers do not have such an understanding. In order to train language models to process we need a mechanism to represent **text features** as features such as words and characters.\r\n",
    "\r\n",
    "# How do we Represent Text with Computers\r\n",
    "\r\n",
    "There are many different approaches to represent textual features in a format that can be modeled with machine learning. We've already mentioned that contextual embeddings is the state of the art but before we explain how contextual embeddigns work lets take a brief tour of the more tradional and early neural approaches for representing text.\r\n",
    "\r\n",
    "## Bag of Words Text Representation\r\n",
    "\r\n",
    "Bag of Words or BoW vector representations are the most common used traditional vector representation. Each word or n-gram is linked to a vector index and marked as 0 or 1 depending on whether it occurs in a given document.\r\n",
    "\r\n",
    "[Put bag of word image here]\r\n",
    "\r\n",
    "BoW representations are often used in methods of document classification where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. One challenge with bag of word representations is that they don’t encode any information with regards to the meaning of a given word.\r\n",
    "\r\n",
    "In BoW word occurrences are evenly weighted independently of how frequently or what context they occur. However in most NLP tasks some words are more relevant than others.\r\n",
    "\r\n",
    "## Term Frequency Inverse Document Frequency TF - IDF\r\n",
    "\r\n",
    "TF - IDF short for term frequency–inverse document frequency, is a variation of bag of words where instead of a binary 0 and 1 value being used to indicate the appearence of a ngram in a document a the TF-IDF value is used. The TF-IDF value is a numerical statstic that reflect how prominent a word or n-gram is to a document in a collection. The TF-IDF value increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently than others. \r\n",
    "\r\n",
    "However even though TF-IDF representations provide frequency weight to different words they are unable to represent meaning or order. As the famous linguist J. R. Firth said in 1935, “The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously.”\r\n",
    "\r\n",
    "## Traditional Distributional Embeddings with Mutual Information\r\n",
    "\r\n",
    "Distributional Embeddings enable word vectors to encapsulate contextual context. Each embedding vector is represented based on the mutual information it has with other words in a given corpus. Mutual information can be represented as a global co-occurrence frequency or restricted to a given window either sequentially or based on dependency edges.\r\n",
    "\r\n",
    "## PreTrained Embeddings Word 2 Vec and Varients\r\n",
    "\r\n",
    "Predictive models learn their vectors in order to improve their predictive ability of a loss such as the loss of predicting the vector for a target word from the vectors of the surrounding context words.\r\n",
    "Word2Vec is a predictive embedding model. There are two main Word2Vec architectures that are used to produce a distributed representation of words:\r\n",
    "\r\n",
    " - Continuous bag-of-words (CBOW) — The order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.\r\n",
    " - Continuous skip-gram weighs nearby context words more heavily than more distant context words. While order still is not captured each of the context vectors are weighed and compared independently vs CBOW which weighs against the average context.\r\n",
    "\r\n",
    "CBOW is faster while skip-gram is slower but does a better job for infrequent words.\r\n",
    "\r\n",
    "Both CBOW and Skip-Grams are “predictive” embeddings, in that they only take local contexts into account. Word2Vec does not take advantage of global context. GloVe embeddings by contrast leverage the same intuition behind the co-occurence matrix used by previous distributional embeddings, but uses neural methods to decompose the co-occurrence matrix into more expressive and non linear word vectors.\r\n",
    "\r\n",
    "FastText, built on Word2Vec by learning vector representations for each word and the charachter n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to pre-training it enables word embeddings to encode sub-word information. \r\n",
    "\r\n",
    "Contextual Embeddings\r\n",
    "\r\n",
    "ELMO\r\n",
    "\r\n",
    "BERT\r\n",
    "\r\n",
    "ETC"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Code example here with hugging face and explain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python38364bit74d6bbee72be4203a51c7d8693e15448"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}