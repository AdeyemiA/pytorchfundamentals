{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In the previous module, we have been using rich semantic representations of text, and a simple linear classifier on top of the embeddings. What this architecture does is to capture aggregated meaning of words in a sentence, but it does not take into account the **order** of words, because aggregation operation on top of embeddings removed this information from the original text. Because these models are unable to model word ordering, they cannot solve more complex or ambiguous tasks such as text generation or question answering.\n",
    "\n",
    "To capture the meaning of text sequence, we need to use another neural network architecture, which is caller **recurrent neural network**, or RNN. In RNN, we pass our sentence through the network one symbol at a time, and the network produces some **state**, which we then pass to the network again with the next symbol.\n",
    "\n",
    "![rnn model](../images/rnn_model.png)\n",
    "\n",
    "Given the input sequence of tokens $X_0,\\dots,X_n$, RNN creates a sequence of neural network blocks, and trains this sequence end-to-end using back propagation. Each network block takes a pair $(X_i,S_i)$ as an input, and produces $S_{i+1}$ as a result. Final state $S_n$ or output $X_n$ goes into a linear classifier to produce the result. All network blocks share the same weights, and are trained end-to-end using one backpropagation pass.\n",
    "\n",
    "Because state vectors $S_0,\\dots,S_n$ are passed through the network, it is able to learn the sequential dependencies between words. For example, when the word *not* appears somewhere in the sequnce, it can learn to negate certain elements within the state vector, resulting in negation.  \n",
    "\n",
    "Let's see how recurrent neural networks can help us classify our news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:04, 27616.79lines/s]\n",
      "120000lines [00:08, 14194.93lines/s]\n",
      "7600lines [00:00, 14547.67lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchnlp import *\n",
    "train_dataset, test_dataset, classes, vocab_size = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN Classifier\n",
    "\n",
    "In case of simple RNN, each recurrent unit is a simple linear network, which takes concatenated input vector and state vector, and produce a new state vector. PyTorch represents this unit with `RNNCell` class, and a networks of such cells - as `RNN` layer.\n",
    "\n",
    "To define an RNN classifier, we will first apply an embedding layer to lower the dimensionality of input vocabulary, and then have RNN layer on top of it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = torch.nn.RNN(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        x,h = self.rnn(x)\n",
    "        return self.fc(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we will use padded data loader, so each batch will have a number of padded sequences of the same length. RNN layer will take the sequence of embedding tensors, and produce two outputs: \n",
    "* $x$ is a sequence of RNN cell outputs at each step\n",
    "* $h$ is a final hidden state for the last element of the sequence\n",
    "\n",
    "We then apply a fully-connected linear classifier to get the number of class.\n",
    "\n",
    "> **Note:** RNNs are quite difficult to train, because once the RNN cells are unrolled along the sequence length, the resulting number of layers involved in back propagation is quite large. Thus we need to select small learning rate, and train the network on larger dataset to produce good results. It can take quite a long time, so using GPU is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.3034375\n",
      "6400: acc=0.3796875\n",
      "9600: acc=0.449375\n",
      "12800: acc=0.50359375\n",
      "16000: acc=0.5425\n",
      "19200: acc=0.57640625\n",
      "22400: acc=0.6054017857142857\n",
      "25600: acc=0.6278515625\n",
      "28800: acc=0.6466319444444445\n",
      "32000: acc=0.66471875\n",
      "35200: acc=0.6782954545454546\n",
      "38400: acc=0.6909375\n",
      "41600: acc=0.7018990384615384\n",
      "44800: acc=0.7116517857142857\n",
      "48000: acc=0.7199166666666666\n",
      "51200: acc=0.72759765625\n",
      "54400: acc=0.735202205882353\n",
      "57600: acc=0.7419791666666666\n",
      "60800: acc=0.748092105263158\n",
      "64000: acc=0.753859375\n",
      "67200: acc=0.7591517857142858\n",
      "70400: acc=0.7640767045454545\n",
      "73600: acc=0.7686277173913043\n",
      "76800: acc=0.7726692708333334\n",
      "80000: acc=0.7769875\n",
      "83200: acc=0.7805408653846154\n",
      "86400: acc=0.7841666666666667\n",
      "89600: acc=0.7874888392857143\n",
      "92800: acc=0.790603448275862\n",
      "96000: acc=0.79353125\n",
      "99200: acc=0.7963407258064517\n",
      "102400: acc=0.7987890625\n",
      "105600: acc=0.80125\n",
      "108800: acc=0.8039797794117647\n",
      "112000: acc=0.8063303571428572\n",
      "115200: acc=0.8084895833333333\n",
      "118400: acc=0.8106503378378378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.03247815348307292, 0.8117333333333333)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)\n",
    "net = RNNClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory (LSTM)\n",
    "\n",
    "One of the main problems of classical RNNs is so-called **vanishing gradients** problem. Because RNNs are trained end-to-end in one back-propagation pass, it is having hard times propagating error to the first layers of the network, and thus the network cannot learn relationships between distant tokens. One of the ways to avoid this problem is to introduce **explicit state management** by using so called **gates**. There are two most known architectures of this kind: **Long Short Term Memory** (LSTM) and **Gated Relay Unit** (GRU).\n",
    "\n",
    "![LSTM Cell](../images/The_LSTM_Cell.svg)\n",
    "\n",
    "LSTM Network is organized in a manner similar to RNN, but there are two states that are being passed from layer to layer: actual state $c$, and hidden vector $h$. At each unit, hidden vector $h_i$ is concatenated with input $x_i$, and they control what happens to the state $c$ via **gates**. Each gate is a neural network with sigmoid activation (output in the range $[0,1]$), which can be thought of as bitwise mask when multiplied by the state vector. There are the following gates (from left to right on the picture above):\n",
    "* **forget gate** takes hidden vector and determines, which components of the vector $c$ we need to forget, and which to pass through. \n",
    "* **input gate** takes some information from the input and hidden vector, and inserts it into state.\n",
    "* **output gate** transforms state via some linear layer with $\\tanh$ activation, then selects some of its components using hidden vector $h_i$ to produce new state $c_{i+1}$.\n",
    "\n",
    "Components of the state $c$ can be thought of as some flags that can be switched on and off. For example, when we encounter a name *Alice* in the sequece, we may want to assume that it refers to female character, and raise the flag in the state that we have female noun in the sentence. When we further encounter phrases *and Tom*, we will raise the flag that we have plural noun. Thus by manipulating state we can supposedly keep track of grammatical properties of sentence parts.\n",
    "\n",
    "While internal structure of LSTM cell may look complex, PyTorch hides this implementation inside `LSTMCell` class, and provides `LSTM` object to represent the whole LSTM layer. Thus, implementation of LSTM classifier will be pretty similar to the simple RNN which we have seen above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-0.5\n",
    "        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        #print(x.size())\n",
    "        x = self.embedding(x)\n",
    "        #print(x.size())\n",
    "        x,(h,c) = self.rnn(x)\n",
    "        #print(x.size(),h.size(),c.size())\n",
    "        return self.fc(h[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our network. Note that training LSTM is also quite slow, and you may not seem much raise in accuracy in the beginning of training. Also, you may need to play with `lr` learning rate parameter to find the learning rate that results in reasonable training speed, and yet does not cause "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.256875\n",
      "6400: acc=0.25953125\n",
      "9600: acc=0.2565625\n",
      "12800: acc=0.25921875\n",
      "16000: acc=0.2606875\n",
      "19200: acc=0.27651041666666665\n",
      "22400: acc=0.298125\n",
      "25600: acc=0.3193359375\n",
      "28800: acc=0.33791666666666664\n",
      "32000: acc=0.3525\n",
      "35200: acc=0.3682102272727273\n",
      "38400: acc=0.3826822916666667\n",
      "41600: acc=0.3998557692307692\n",
      "44800: acc=0.41964285714285715\n",
      "48000: acc=0.43966666666666665\n",
      "51200: acc=0.46169921875\n",
      "54400: acc=0.48270220588235296\n",
      "57600: acc=0.5016145833333333\n",
      "60800: acc=0.5197532894736843\n",
      "64000: acc=0.53575\n",
      "67200: acc=0.5507440476190476\n",
      "70400: acc=0.5643465909090909\n",
      "73600: acc=0.5772282608695652\n",
      "76800: acc=0.5891536458333333\n",
      "80000: acc=0.6009375\n",
      "83200: acc=0.6115745192307692\n",
      "86400: acc=0.6214467592592593\n",
      "89600: acc=0.6308482142857142\n",
      "92800: acc=0.6395689655172414\n",
      "96000: acc=0.6476354166666667\n",
      "99200: acc=0.6549697580645162\n",
      "102400: acc=0.662197265625\n",
      "105600: acc=0.6688352272727273\n",
      "108800: acc=0.6752941176470588\n",
      "112000: acc=0.6816339285714286\n",
      "115200: acc=0.6874652777777778\n",
      "118400: acc=0.6928209459459459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.04210443115234375, 0.6955833333333333)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LSTMClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packed Sequences\n",
    "\n",
    "In our example, we had to pad all sequences in the minibatch with zero vectors. While it results in some memory waste, with RNNs it is more critical that additional RNN cells are created for the padded input items, which take part in training, yet do not carry any important input information. It would be much better to train RNN only to the actual sequence size.\n",
    "\n",
    "To do that, a special format of padded sequence storage is introduced in PyTorch. Suppose we have input padded minibatch which looks like this:\n",
    "```\n",
    "[[1,2,3,4,5],\n",
    " [6,7,8,0,0],\n",
    " [9,0,0,0,0]]\n",
    "```\n",
    "Here 0 represents padded values, and the actual length vector of input sequences is `[5,3,1]`.\n",
    "\n",
    "In order to effectively train RNN with padded sequence, we want to begin training first group of RNN cells with large minibatch (`[1,6,9]`), but then end processing of third sequence, and continue training with shorted minibatches (`[2,7]`, `[3,8]`), and so on. Thus, packed sequence is represented as one vector - in our case `[1,6,9,2,7,3,8,4,5]`, and length vector (`[5,3,1]`), from which we can easily reconstruct the original padded minibatch.\n",
    "\n",
    "To produce packed sequence, we can use `torch.nn.utils.rnn.pack_padded_sequence` function. All recurrent layers, including RNN, LSTM and GRU, support packed sequences as input, and produce packed output, which can be decoded using `torch.nn.utils.rnn.pad_packed_sequence`.\n",
    "\n",
    "To be able to produce packed sequence, we need to pass length vector to the network, and thus we need a different function to prepare minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_length(b):\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    len_seq = [len(x[1]) for x in b]\n",
    "    l = max(len_seq)\n",
    "    return ( # tuple of three tensors - labels, padded features, length sequence\n",
    "        torch.LongTensor([t[0] for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t[1]),(0,l-len(t[1])),mode='constant',value=0) for t in b]),\n",
    "        torch.tensor(len_seq)\n",
    "    )\n",
    "\n",
    "train_loader_len = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=pad_length, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual network would be very similar to `LSTMClassifier` above, but `forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPackClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data)-0.5\n",
    "        self.rnn = torch.nn.LSTM(embed_dim,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.embedding(x)\n",
    "        pad_x = torch.nn.utils.rnn.pack_padded_sequence(x,lengths,batch_first=True,enforce_sorted=False)\n",
    "        pad_x,(h,c) = self.rnn(pad_x)\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(pad_x,batch_first=True)\n",
    "        #print(x.size(),h.size(),c.size())\n",
    "        return self.fc(h[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.285625\n",
      "6400: acc=0.35359375\n",
      "9600: acc=0.4163541666666667\n",
      "12800: acc=0.466875\n",
      "16000: acc=0.513375\n",
      "19200: acc=0.5499479166666666\n",
      "22400: acc=0.5829017857142857\n",
      "25600: acc=0.6090234375\n",
      "28800: acc=0.6328125\n",
      "32000: acc=0.65221875\n",
      "35200: acc=0.6679829545454545\n",
      "38400: acc=0.68265625\n",
      "41600: acc=0.6958413461538462\n",
      "44800: acc=0.70796875\n",
      "48000: acc=0.717875\n",
      "51200: acc=0.72654296875\n",
      "54400: acc=0.7354411764705883\n",
      "57600: acc=0.7426388888888888\n",
      "60800: acc=0.7495888157894737\n",
      "64000: acc=0.75584375\n",
      "67200: acc=0.7616369047619047\n",
      "70400: acc=0.7670738636363637\n",
      "73600: acc=0.7718070652173913\n",
      "76800: acc=0.7765755208333334\n",
      "80000: acc=0.780925\n",
      "83200: acc=0.7847235576923077\n",
      "86400: acc=0.7886921296296296\n",
      "89600: acc=0.7923102678571429\n",
      "92800: acc=0.7954202586206897\n",
      "96000: acc=0.7985416666666667\n",
      "99200: acc=0.8016633064516129\n",
      "102400: acc=0.804658203125\n",
      "105600: acc=0.8075189393939394\n",
      "108800: acc=0.8099816176470588\n",
      "112000: acc=0.8122767857142857\n",
      "115200: acc=0.8149479166666667\n",
      "118400: acc=0.8172972972972973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.02941259562174479, 0.8183416666666666)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LSTMPackClassifier(vocab_size,64,32,len(classes)).to(device)\n",
    "train_epoch_emb(net,train_loader_len, lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1lines [00:00, 17.30lines/s]\n",
      "120000lines [00:08, 13428.63lines/s]\n",
      "7600lines [00:00, 8951.09lines/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.25125\n",
      "6400: acc=0.25\n",
      "9600: acc=0.248125\n",
      "12800: acc=0.246875\n",
      "16000: acc=0.24725\n",
      "19200: acc=0.24713541666666666\n",
      "22400: acc=0.24607142857142858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.42084818014485365, 0.2473608445297505)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "# build new `Vocab` object from GloVe vocabulary and load embedding vectors \n",
    "voc = torchtext.vocab.build_vocab_from_iterator([vocab.itos])\n",
    "voc.load_vectors(vocab)\n",
    "# load the dataset using pre-defined vocabulary\n",
    "train_dataset, test_dataset = torchtext.datasets.text_classification.DATASETS['AG_NEWS'](root='./data', vocab=voc)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.249375\n",
      "6400: acc=0.25\n",
      "9600: acc=0.2504166666666667\n",
      "12800: acc=0.249609375\n",
      "16000: acc=0.25\n",
      "19200: acc=0.24963541666666667\n",
      "22400: acc=0.24977678571428572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.640533779190659, 0.2492802303262956)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = RNNClassifier(vocab_size,50,32,len(classes))\n",
    "net.embedding.weight.data = voc.vectors\n",
    "net = net.to(device)\n",
    "train_epoch(net,train_loader, lr=10, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (LSTMs and GRU)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short Term Memory Cells (LSTMs) and Gated Recurrent Units (GRUs) provided a mechanism for modeling word ordering by forwarding the context of each previous prediction into the next evaluation step.\n",
    "\n",
    "\n",
    "\n",
    "This enabled more complex natrual language processing tasks that require sequence to sequence as well as encoder/decoder mechanisms to be more effectively modeled with neural frameworks such as PyTorch such as Text Translation, Image Captioning, and Named Entity Recognition.\n",
    "\n",
    "The image below showcases some of the neural tasks that RNNs enabled with neural methods.\n",
    "\n",
    "![RNN paterns](../images/rnn_tasks.gif)\n",
    "\n",
    "Additional variations of RNNs such as Bidirectional-RNNs which process text in both left to right and right to left and character level RNNs for enhancing underrepresented or out of vocabulary word embeddings led to many state of the art neural NLP breakthroughs.\n",
    "\n",
    "One cause for sub-optimal performance with standard LSTM encoder-decoder models for sequence to sequence tasks such as Named Entity Recognition and Machine Translation is that they weighted the impact each input vector evenly on each output vector. In reality specific words in the input sequence often have more impact on sequential outputs at different time steps.\n",
    "\n",
    "## Attention Mechanisms\n",
    "\n",
    "**Attention Mechanisms** provide a means of weighting the contextual impact of each input vector on each output prediction of the RNN. \n",
    "\n",
    "![attention](images/attention.gif)\n",
    "\n",
    "An example of an attention mechanism applied to the task of neural translation in Microsoft Translator\n",
    "\n",
    "Attention mechanisms are responsible for much of the current or near current state of the art in Natural language processing. Adding attention however greatly increases the number of model parameters which led to scaling issues with RNNs. A key constraint of scaling RNNS is that the recurrent nature of the models makes it challenging to batch and parelleize training. In an RNN each element of a sequence needs to be processed in sequential order which means it cannot be easily parallelized.\n",
    "\n",
    "This adoption of attention mechanisms combined with this constraint led to the creation of the now State of the Art Transformer Models that we know and use today from BERT to OpenGPT3.\n",
    "\n",
    "## Tranformer Models\n",
    "\n",
    "Instead of forwarding the context of each previous prediction into the next evaluation step Transformer models use positonal encodings and attention to capture the context of a given input with in a provided window size of text. The image below shows how the positional encodings with attention can capture context with in a given window.\n",
    "\n",
    "![](images/transformer_explination.gif) \n",
    "\n",
    "\n",
    "Since each input position is mapped independently to each output position, transformers can parallelize better than RNNs which enables much larger and more expressive language models. Each attention head can be used to learn different relationships between words that improves downstream Natrual Language Processing tasks.\n",
    "\n",
    "BERT is a very large multi layer transformer network with (12 layers for BERT-base, and 24 for BERT-large). The model is first pre-trained on large corpus of text data (WikiPedia + books) using un-superwised training (predicting masked words in a sentence). During pre-training the model absorbs significant level of language understanding which can then be leveraged with other datasets using fine tuning. This process is called **transfer learning**. \n",
    "\n",
    "![picture from http://jalammar.github.io/illustrated-bert/](images/jalammarBERT-language-modeling-masked-lm.png)\n",
    "\n",
    "There are many variations of Transformer architectures including BERT, DistilBERT. BigBird, OpenGPT3 and more that can be fine tuned. The HuggingFace package provides repository for training many of these architectures with PyTorch. \n",
    "\n",
    "![HuggingFace](images/huggingface.jpg)\n",
    "\n",
    "In the next module we will be using the HuggingFace Library with PyTorch to fine tune a state of the art DistilBert transformer model for question and answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
