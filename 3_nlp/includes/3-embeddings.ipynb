{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Embeddings\n",
    "\n",
    "In our previous example, we operated on high-dimensional bag-of-words vectors with length `vocab_size`, and we were explicitly converting from low-dimensional positional representation vectors into sparse one-hot representation. This one-hot representation is not very efficient, and each word is treated independently from each other, i.e. one-hot encoded vectors do not show any semantic similarity between words.\n",
    "\n",
    "The idea of **embedding** is to represent words by lower-dimensional dense vectors, which somehow reflect semantic meaning of a word. We will later discuss how to build meaningful word embeddings, but for now let's just think of embeddings as a way to lower dimensionality of a word vector. \n",
    "\n",
    "So, embedding layer would take a word as an input, and produce an output vector of specified `embedding_size`. In a sense, it is very similar to `Linear` layer, but instead of taking one-hot encoded vector, it will be able to take a word number as an input.\n",
    "\n",
    "By using embedding layer as a first layer in our network, we can switch from bag-or-words to **embedding bag** model, where we first convert each word in our text into corresponding embedding, and then compute some aggregate function over all those embeddings, such as `sum`, `average` or `max`.  \n",
    "\n",
    "![Embedding Classifier](../images/embed-classifier.png)\n",
    "\n",
    "As a result of this architecture, minibatches to our network would need to be created in a certain way. In the previous example, all BoW tensors in a minibatch had equal size `vocab_size`, regardless of the actual length of a sequence. In  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "\n",
    "## PreTrained Embeddings Word2Vec and Varients\n",
    "\n",
    "As opposed to traditional distributional models neural embeddings such as Word to Vec are learned by training a neural langauge model to minimize a loss function for tasks that map to language understanding.  This process of training models on large collections of text to extract word representaions is called pre-training.  \n",
    "\n",
    "One of the first sucessful neural pretraining techniques for text representation was called Word2Vec. \n",
    "\n",
    "There are two main architectures that are used to produce a distributed representation of words:\n",
    "\n",
    " - Continuous bag-of-words (CBOW) — In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.\n",
    " - Continuous skip-gram - In the continuous skip-gram architecture, the model uses surrounding window of context words to predict the current word.\n",
    "\n",
    "CBOW is faster while skip-gram is slower but does a better job of representing infrequent words.\n",
    "\n",
    "![word2vec image](../images/word2vec.png)\n",
    "\n",
    "Both CBOW and Skip-Grams are “predictive” embeddings, in that they only take local contexts into account. Word2Vec does not take advantage of global context. FastText, built on Word2Vec by learning vector representations for each word and the charachter n-grams found within each word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to pre-training it enables word embeddings to encode sub-word information. \n",
    "\n",
    "Another method called GloVe by contrast leverages the same intuition behind the co-occurence matrix used by the traditional distributional embeddings above, but uses neural methods to decompose the co-occurrence matrix into more expressive and non linear word vectors.\n",
    "\n",
    "Below we can use the Gensim Api to play with pretrained word2vec, fast text, and glove embeddings to find the most similar pretrained embeddings to the word 'play':\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')\n",
    "print(w2v.most_similar('play'))\n",
    "\n",
    "from gensim.models.wrappers import FastText\n",
    "fast_text = FastText.load_fasttext_format('wiki.simple')\n",
    "print(fast_text.most_similar('play'))\n",
    "\n",
    "glove = api.load(\"glove-twitter-25\")\n",
    "print(glove.most_similar('play'))"
   ]
  },
  {
   "source": [
    "\n",
    "One key limitation of tradition pretrained embedding representaitons such as Word2Vec is the problem of word sense disambigioution. While pretrained embeddings can capture some of the meaning of words in context every possible meaning of a word is encoded into the same embedding. This can cause problems in downstream models since many words such as the word 'play' have different meanings depending on the context they are used in.\n",
    "\n",
    "For example word 'play' in the the sentence\n",
    "- I went to a [play] at the theature.\n",
    "\n",
    "Does not mean the same thing as the word 'play' in the sentence.\n",
    "- John wants to [play] with his friends.\n",
    "\n",
    "The pretrained embeddings above represent both of these meanings of the word 'play' in the same embedding. Contextual embeddings ,methods were developed to address this challenge of disambigutation and contributed to the massive leap forward in natrual language processing applications. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "## Contextual Embeddings\n",
    "\n",
    "To address challenges of word sense disambigution a new method of pretraining models on large amounts of data and using the pre-trained models to generate contextual embeddings was spearheaded with the advent of models such as ULMFiT, ELMO and Later BERT.\n",
    "\n",
    "![elmo](images/elmo.png)\n",
    "\n",
    "Below we will look at Spacy's transformer api to play with contextual embeddings.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy-transformers\n",
    "!python -m spacy download \"en_trf_bertbaseuncased_lg\"\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "doc1 = nlp(\"I went to a play.\")\n",
    "doc2 = nlp(\"John wants to play a game.\")\n",
    "doc3 = nlp(\"John went to a show.\")\n",
    "\n",
    "\n",
    "print(\"Similarity between the two words 'play' in doc1 and doc2:\", doc1[4].similarity(doc2[3]))\n",
    "print(\"Similarity between doc1 'play' and doc3 'show':\", doc1[4].similarity(doc3[4]))\n",
    "print(\"Similarity between doc2 'play' and doc3 'show':\", doc2[3].similarity(doc3[4]))"
   ]
  },
  {
   "source": [
    "ULMFit and ELMo were models that generates embedding for a word based on the context it appears thus generating slightly different embeddings for each of its occurrence and thus alowing a downstream model to better disambiguate between the correct sense of a given word such as 'play'. On in it’s release it enabled near instant state of the art results in many downstream tasks, including tasks such as co-reference were previously not as viable for practical usage in nlp.\n",
    "\n",
    "This was coined as the ImageNet moment of NLP more recent transfomer based models such as BERT capitalize on the development of BERT using attention transformers instead of bi-directonal RNNs to encode context. If you are unfamiliar with terms such as Transformers and RNNs do not worry in the next module we will walk through the progression of NLP models culminiating in the advent of current state of the models in NLP with PyTorch. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}