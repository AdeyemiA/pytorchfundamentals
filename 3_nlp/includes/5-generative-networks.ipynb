{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short Term Memory Cells (LSTMs) and Gated Recurrent Units (GRUs) provided a mechanism for language modeling, i.e. they can learn word ordering and provide predictions for the next word in a sequence. This allows us to use RNNs for **generative tasks**, such as ordinary text generation, machine translation, and even image captioning.\n",
    "\n",
    "In the RNN architecture we discussed in the previous unit, each RNN unit produced next next hidden state as an output. However, we can also add another output to each recurrent unit, which would allow us to output a **sequence** (which is equal in length to the original sequence). Moreover, we can use RNN units that do not accept an input at each step, and just take some initial state vector, and then produce a sequence of outputs.\n",
    "\n",
    "This allows for different neural architectures that are shown in the picture below:\n",
    "\n",
    "![RNN paterns](../images/unreasonable-effectiveness-of-rnn.jpg)\n",
    "*Image from blog post [Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by [Andrej Karpaty](http://karpathy.github.io/)*\n",
    "\n",
    "* **One-to-one** is a traditional neural network with one input and one output\n",
    "* **One-to-many** is a generative architecture that accepts one input value, and generates a sequence of output values. For example, if we want to train **image captioning** network that would produce a textual description of a picture, we can a picture as input, pass it through CNN to obtain hidden state, and then have recurrent chain generate caption word-by-word\n",
    "* **Many-to-one** corresponds to RNN architectures we described in the previous unit, such as text classification\n",
    "* **Many-to-many**, or **sequence-to-sequence** corresponds to tasks such as **machine translation**, where we have first RNN collect all information from the input sequence into the hidden state, and another RNN chain unrolls this state into the output sequence.\n",
    "\n",
    "In this unit, we will focus on simple generative models that help us generate text. For simplicity, let's build **character-level network**, in which we will take individual characters as an input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:04, 27589.41lines/s]\n",
      "120000lines [00:08, 14627.95lines/s]\n",
      "7600lines [00:00, 14564.78lines/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torchtext.datasets.text_classification.TextClassificationDataset at 0x7f21e8ef7990>,\n",
       " <torchtext.datasets.text_classification.TextClassificationDataset at 0x7f21df1fd190>,\n",
       " ['World', 'Sports', 'Business', 'Sci/Tech'],\n",
       " 95812)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "load_dataset() # we need this to make sure data is fetched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenizer(words):\n",
    "    return list(words) #[word for word in words]\n",
    "\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=char_tokenizer) #, lower=True)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train_dataset = torchtext.data.TabularDataset('./data/ag_news_csv/train.csv',\n",
    "        format='csv',\n",
    "        fields=[('Label', LABEL), ('Head', TEXT), ('Text', TEXT) ])\n",
    "TEXT.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 84\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(TEXT.vocab)\n",
    "print(f\"Vocabulary size = {vocab_size}\")\n",
    "\n",
    "nchars = 100\n",
    "\n",
    "def encode_text(s):\n",
    "    return torch.LongTensor([TEXT.vocab.stoi[t] for t in s])\n",
    "\n",
    "def get_batches(s,batch_size=16,nchars=nchars):\n",
    "    ins = torch.zeros(len(s)-nchars,nchars,dtype=torch.long)\n",
    "    outs = torch.zeros(len(s)-nchars,nchars,dtype=torch.long)\n",
    "    for i in range(len(s)-nchars):\n",
    "        ins[i] = encode_text(s[i:i+nchars])\n",
    "        outs[i] = encode_text(s[i+1:i+nchars+1])\n",
    "    return ins,outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'e', 'u', 't', 'e', 'r', 's', ' ', '-', ' ', 'S', 'h', 'o', 'r', 't', '-', 's', 'e', 'l', 'l', 'e', 'r', 's', ',', ' ', 'W', 'a', 'l', 'l', ' ', 'S', 't', 'r', 'e', 'e', 't', \"'\", 's', ' ', 'd', 'w', 'i', 'n', 'd', 'l', 'i', 'n', 'g', '\\\\', 'b', 'a', 'n', 'd', ' ', 'o', 'f', ' ', 'u', 'l', 't', 'r', 'a', '-', 'c', 'y', 'n', 'i', 'c', 's', ',', ' ', 'a', 'r', 'e', ' ', 's', 'e', 'e', 'i', 'n', 'g', ' ', 'g', 'r', 'e', 'e', 'n', ' ', 'a', 'g', 'a', 'i', 'n', '.']\n"
     ]
    }
   ],
   "source": [
    "for i,x in zip(range(1),train_dataset.examples):\n",
    "    print(x.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGenerator(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = torch.nn.LSTM(vocab_size,hidden_dim,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, s=None):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.nn.functional.one_hot(x,vocab_size).to(torch.float32)\n",
    "        x,s = self.rnn(x,s)\n",
    "        return self.fc(x),s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(net,size=100,start='today '):\n",
    "        chars = list(start)\n",
    "        out, s = net(encode_text(chars).view(1,-1).to(device))\n",
    "        for i in range(size):\n",
    "            nc = torch.argmax(out[0][-1])\n",
    "            chars.append(TEXT.vocab.itos[nc])\n",
    "            out, s = net(nc.view(1,-1),s)\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss = 1.9617218971252441\n",
      "today a the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "Current loss = 1.5846630334854126\n",
      "today and the company and the company and the company and the company and the company and the company and \n",
      "Current loss = 2.368056058883667\n",
      "today and the proporting the proporting the proporting the proporting the proporting the proporting the pr\n",
      "Current loss = 1.7323575019836426\n",
      "today and the contruduction to the to stock to the to stock to the to stock to the to stock to the to stoc\n",
      "Current loss = 1.6278003454208374\n",
      "today and the U.S. compaling the compaling the compaling the compaling the compaling the compaling the com\n",
      "Current loss = 1.9082142114639282\n",
      "today and the to the to the to the to the to the to the to the to the to the to the to the to the to the t\n",
      "Current loss = 1.6412254571914673\n",
      "today and the company and the company and the company and the company and the company and the company and \n",
      "Current loss = 1.6157582998275757\n",
      "today and the company and the company and the company and the company and the company and the company and \n",
      "Current loss = 1.3933299779891968\n",
      "today and the company the company the company the company the company the company the company the company \n",
      "Current loss = 1.5915017127990723\n",
      "today to the start to the start to the start to the start to the start to the start to the start to the st\n",
      "Current loss = 1.9431548118591309\n",
      "today the standation to the standation to the standation to the standation to the standation to the standa\n",
      "Current loss = 1.6908080577850342\n",
      "today and the second the ternation of the second the ternation of the second the ternation of the second t\n",
      "Current loss = 1.9907022714614868\n",
      "today to the second to the second to the second to the second to the second to the second to the second to\n",
      "Current loss = 2.2510406970977783\n",
      "today and the security of the security of the security of the security of the security of the security of \n",
      "Current loss = 1.9585249423980713\n",
      "today and the provide to the provide to the provide to the provide to the provide to the provide to the pr\n",
      "Current loss = 1.4927181005477905\n",
      "today to the start of the start of the start of the start of the start of the start of the start of the st\n",
      "Current loss = 1.6475318670272827\n",
      "today to the street the street the street the street the street the street the street the street the stree\n",
      "Current loss = 1.4972246885299683\n",
      "today and the second to the season and the season and the season and the season and the season and the sea\n",
      "Current loss = 1.5830351114273071\n",
      "today and the start of the start of the start of the start of the start of the start of the start of the s\n",
      "Current loss = 1.8865923881530762\n",
      "today and the strepter of the strepter of the strepter of the strepter of the strepter of the strepter of \n",
      "Current loss = 1.7417641878128052\n",
      "today and the season to the services and the season to the services and the season to the services and the\n",
      "Current loss = 1.6560161113739014\n",
      "today and the security that the security that the security that the security that the security that the se\n",
      "Current loss = 1.998162865638733\n",
      "today and the season to the season to the season to the season to the season to the season to the season t\n",
      "Current loss = 1.9230376482009888\n",
      "today to the No. 11 State and the No. 11 State and the No. 11 State and the No. 11 State and the No. 11 St\n",
      "Current loss = 2.077183485031128\n",
      "today and the state the state the state the state the state the state the state the state the state the st\n",
      "Current loss = 1.1011910438537598\n",
      "today to the second the second the second the second the second the second the second the second the secon\n",
      "Current loss = 2.4008519649505615\n",
      "today to the season to parting the season to parting the season to parting the season to parting the seaso\n",
      "Current loss = 1.563717246055603\n",
      "today to the second to the second to the second to the second to the second to the second to the second to\n",
      "Current loss = 1.4492826461791992\n",
      "today and the a security and the a security and the a security and the a security and the a security and t\n",
      "Current loss = 1.8736969232559204\n",
      "today and the and the and the and the and the and the and the and the and the and the and the and the and \n",
      "Current loss = 1.5065736770629883\n",
      "today and the start the start the start the start the start the start the start the start the start the st\n",
      "Current loss = 1.8683898448944092\n",
      "today of the start to the start to the start to the start to the start to the start to the start to the st\n",
      "Current loss = 1.612565517425537\n",
      "today and the start of the start of the start of the start of the start of the start of the start of the s\n",
      "Current loss = 1.750881314277649\n",
      "today that the security of the security of the security of the security of the security of the security of\n",
      "Current loss = 2.033127546310425\n",
      "today to the started the started the started the started the started the started the started the started t\n",
      "Current loss = 1.4532513618469238\n",
      "today to the second to the second to the second to the second to the second to the second to the second to\n",
      "Current loss = 1.3750393390655518\n",
      "today and the Universe and the Universe and the Universe and the Universe and the Universe and the Univers\n",
      "Current loss = 1.9432212114334106\n",
      "today to the start of the start of the start of the start of the start of the start of the start of the st\n",
      "Current loss = 1.3800166845321655\n",
      "today the second the second the second the second the second the second the second the second the second t\n",
      "Current loss = 1.529610276222229\n",
      "today and the started the started the started the started the started the started the started the started \n",
      "Current loss = 2.055208921432495\n",
      "today and the second the second the second the second the second the second the second the second the seco\n",
      "Current loss = 1.703906536102295\n",
      "today and the second than the second than the second than the second than the second than the second than \n",
      "Current loss = 1.564253807067871\n",
      "today and the second the second the second the second the second the second the second the second the seco\n",
      "Current loss = 1.4914640188217163\n",
      "today and the securition and the securitical service to the securition and the securitical service to the \n",
      "Current loss = 1.582850456237793\n",
      "today and the security to the security to the security to the security to the security to the security to \n",
      "Current loss = 1.710318922996521\n",
      "today and the second to the second to the second to the second to the second to the second to the second t\n",
      "Current loss = 1.598741888999939\n",
      "today and the started the started the started the started the started the started the started the started \n",
      "Current loss = 1.404415488243103\n",
      "today and the started the started the started the started the started the started the started the started \n",
      "Current loss = 1.7114897966384888\n",
      "today and the season of the season of the season of the season of the season of the season of the season o\n",
      "Current loss = 1.7730814218521118\n",
      "today and the services and the services and the services and the services and the services and the service\n",
      "Current loss = 1.4766507148742676\n",
      "today and the second to the second to the second to the second to the second to the second to the second t\n",
      "Current loss = 1.2704789638519287\n",
      "today and the than the starting the starting the starting the starting the starting the starting the start\n",
      "Current loss = 1.5304094552993774\n",
      "today and the second to the second to the second to the second to the second to the second to the second t\n",
      "Current loss = 1.4465528726577759\n",
      "today and the service the service the service the service the service the service the service the service \n",
      "Current loss = 1.478875994682312\n",
      "today and the service to the service to the service to the service to the service to the service to the se\n",
      "Current loss = 1.7706387042999268\n",
      "today and the second to the second to the second to the second to the second to the second to the second t\n",
      "Current loss = 1.8671926259994507\n",
      "today and the service to the service to the service to the service to the service to the service to the se\n",
      "Current loss = 1.7393709421157837\n",
      "today and the season of the season of the season of the season of the season of the season of the season o\n",
      "Current loss = 1.657270908355713\n",
      "today and the security of the security of the security of the security of the security of the security of \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss = 1.8670830726623535\n",
      "today to the second to the second to the second to the second to the second to the second to the second to\n",
      "Current loss = 2.014064073562622\n",
      "today and the state to the state to the state to the state to the state to the state to the state to the s\n",
      "Current loss = 1.6249018907546997\n",
      "today and the services and the services and the services and the services and the services and the service\n",
      "Current loss = 1.8014146089553833\n",
      "today and the security of the security of the security of the security of the security of the security of \n",
      "Current loss = 1.6114312410354614\n",
      "today and the second to the second to the second to the second to the second to the second to the second t\n",
      "Current loss = 1.5414443016052246\n",
      "today and the state to the state to the state to the state to the state to the state to the state to the s\n",
      "Current loss = 1.742472767829895\n",
      "today and the second the second the second the second the second the second the second the second the seco\n",
      "Current loss = 1.5987434387207031\n",
      "today and the struggled the struggled the struggled the struggled the struggled the struggled the struggle\n",
      "Current loss = 1.764091968536377\n",
      "today and the security and the security and the security and the security and the security and the securit\n",
      "Current loss = 1.8133114576339722\n",
      "today and the season and the season and the season and the season and the season and the season and the se\n",
      "Current loss = 1.4042983055114746\n",
      "today and the season of the season of the season of the season of the season of the season of the season o\n",
      "Current loss = 1.6645481586456299\n",
      "today and the search a search and the search a search and the search a search and the search a search and \n",
      "Current loss = 1.4748308658599854\n",
      "today and the service the service the service the service the service the service the service the service \n",
      "Current loss = 1.882836937904358\n",
      "today and a strike a strike a strike a strike a strike a strike a strike a strike a strike a strike a stri\n",
      "Current loss = 1.6282737255096436\n",
      "today and the start to the start to the start to the start to the start to the start to the start to the s\n",
      "Current loss = 1.7006940841674805\n",
      "today and the state to the state to the state to the state to the state to the state to the state to the s\n",
      "Current loss = 1.560899019241333\n",
      "today and the started the started the started the started the started the started the started the started \n",
      "Current loss = 1.6034088134765625\n",
      "today and the server to the server to the server to the server to the server to the server to the server t\n",
      "Current loss = 1.5495226383209229\n",
      "today and the state the state the state the state the state the state the state the state the state the st\n",
      "Current loss = 1.8970292806625366\n",
      "today and the search the search the search the search the search the search the search the search the sear\n",
      "Current loss = 1.6215722560882568\n",
      "today and the state to the state to the state to the state to the state to the state to the state to the s\n",
      "Current loss = 1.6589345932006836\n",
      "today and the company and the company and the company and the company and the company and the company and \n",
      "Current loss = 1.6996866464614868\n",
      "today and the stand of the stand of the stand of the stand of the stand of the stand of the stand of the s\n",
      "Current loss = 1.442850112915039\n",
      "today and the season and the season and the season and the season and the season and the season and the se\n",
      "Current loss = 1.3953200578689575\n",
      "today and the start to the start to the start to the start to the start to the start to the start to the s\n",
      "Current loss = 1.4934108257293701\n",
      "today and the second the second the second the second the second the second the second the second the seco\n",
      "Current loss = 1.5235521793365479\n",
      "today to the season to the season to the season to the season to the season to the season to the season to\n",
      "Current loss = 1.5694818496704102\n",
      "today and the state the state the state the state the state the state the state the state the state the st\n",
      "Current loss = 1.574307918548584\n",
      "today and the start to the straight the start to the straight the start to the straight the start to the s\n",
      "Current loss = 1.4584887027740479\n",
      "today and the season of the season of the season of the season of the season of the season of the season o\n",
      "Current loss = 1.5152298212051392\n",
      "today and the state of the first street and the state of the first street and the state of the first stree\n",
      "Current loss = 1.8222746849060059\n",
      "today and the second the second the second the second the second the second the second the second the seco\n",
      "Current loss = 1.363088607788086\n",
      "today and the security to the security to the security to the security to the security to the security to \n",
      "Current loss = 1.7408617734909058\n",
      "today to second to season to second to season to second to season to second to season to second to season \n",
      "Current loss = 1.7114008665084839\n",
      "today and the state of the state of the state of the state of the state of the state of the state of the s\n",
      "Current loss = 1.5838298797607422\n",
      "today and the second to be the second to be the second to be the second to be the second to be the second \n",
      "Current loss = 1.8866208791732788\n",
      "today and the state to the state to the state to the state to the state to the state to the state to the s\n",
      "Current loss = 1.9179580211639404\n",
      "today and the services and the services and the services and the services and the services and the service\n",
      "Current loss = 1.5895986557006836\n",
      "today and the start and the start and the start and the start and the start and the start and the start an\n",
      "Current loss = 1.5033156871795654\n",
      "today and the start to the start to the start to the start to the start to the start to the start to the s\n",
      "Current loss = 1.490393042564392\n",
      "today and the state to the state to the state to the state to the state to the state to the state to the s\n",
      "Current loss = 1.6083557605743408\n",
      "today and the second to the season of the second to the season of the second to the season of the second t\n",
      "Current loss = 1.8273178339004517\n",
      "today and the second search the search the search the search the search the search the search the search t\n",
      "Current loss = 1.434252142906189\n",
      "today and the season of the season of the season of the season of the season of the season of the season o\n",
      "Current loss = 1.5563286542892456\n",
      "today and the second to the second to the second to the second to the second to the second to the second t\n",
      "Current loss = 1.314418911933899\n",
      "today and the search to the top of the search to the top of the search to the top of the search to the top\n",
      "Current loss = 1.7399015426635742\n",
      "today and the second the search the second the search the second the search the second the search the seco\n",
      "Current loss = 1.5504010915756226\n",
      "today and the US to the season and the US to the season and the US to the season and the US to the season \n",
      "Current loss = 1.4878658056259155\n",
      "today the streend to the search the street the street the street the street the street the street the stre\n",
      "Current loss = 1.3195984363555908\n",
      "today and the second the second the second the second the second the second the second the second the seco\n"
     ]
    }
   ],
   "source": [
    "net = LSTMGenerator(vocab_size,64).to(device)\n",
    "    \n",
    "optimizer = torch.optim.Adam(net.parameters(),0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "net.train()\n",
    "for i,x in enumerate(train_dataset.examples):\n",
    "    if len(x.Text)-nchars<10:\n",
    "        continue\n",
    "    text_in, text_out = get_batches(x.Text)\n",
    "    optimizer.zero_grad()\n",
    "    text_in, text_out = text_in.to(device), text_out.to(device)\n",
    "    out,s = net(text_in)\n",
    "    loss = torch.nn.functional.cross_entropy(out.view(-1,vocab_size),text_out.flatten()) #cross_entropy(out,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%1000==0:\n",
    "        print(f\"Current loss = {loss.item()}\")\n",
    "        print(generate(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
