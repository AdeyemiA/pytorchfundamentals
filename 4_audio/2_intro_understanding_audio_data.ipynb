{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Audio Data using torchaudio\n",
    "\n",
    "We need to first get a better understanding of our audio data. Here we will look at some key concepts and features of audio data, how we can visualize and transform the data.\n",
    "\n",
    "\n",
    "TODO torchaudio intro stuff here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages\n",
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the audio backend?\n",
    "\n",
    "The audio backend provide I/O functions to work with and play audio files. If you are running this notebook on Windows we need to switch the audio backend to `soundfile`. The default `sox` audio backend is not support on windows. If you are running on Linux or Mac you can skip this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soundfile'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchaudio.set_audio_backend('soundfile')\n",
    "str(torchaudio.get_audio_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset\n",
    "\n",
    "PyTorch has a variety of datasets built in, we will use on of them called the [Speech Commands](). We will download the full dataset but we are going to only use the yes and no classes to create a binary classification model. First lets download that dataset before we jump into the key concepts and terms to help us understand and work with audio data.\n",
    "\n",
    "After the dataset is download we will visualze all the classes available in the dataset and loop through to create the yes and no collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speech commands dataset has different words - we are going to grab yes and no\n",
    "trainset_speechcommands = torchaudio.datasets.SPEECHCOMMANDS('./data', download=True)\n",
    "#figure out how to parse out classes in data loader? except folders?\n",
    "#train_speechcommands_loader = torch.utils.data.DataLoader(trainset_speechcommands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\code\\pytorchfundamentals\\4_audio\n",
      "['backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow', 'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero', '_background_noise_']\n"
     ]
    }
   ],
   "source": [
    "# get current directory and save as default\n",
    "default_dir = os.getcwd() \n",
    "print(default_dir)\n",
    "\n",
    "os.chdir('./data/SpeechCommands/speech_commands_v0.02/')\n",
    "labels = [name for name in os.listdir('.') if os.path.isdir(name)]\n",
    "# back to default directory\n",
    "os.chdir(default_dir)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_yes = torch.utils.data.DataLoader('./data/SpeechCommands/speech_commands_v0.02/yes', batch_size=1,\n",
    "                                            shuffle=True, num_workers=0)\n",
    "trainloader_no = torch.utils.data.DataLoader('./data/SpeechCommands/speech_commands_v0.02/no', batch_size=1,\n",
    "                                            shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/SpeechCommands/speech_commands_v0.02/yes'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader_yes.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAVE file\n",
    "\n",
    "You move likely have used a wave file before and undrestand that this is the format in which we save our digital representation of our analog audio to be shared and played. The Speech Commands dataset that we are using for this tutorial is stored in wave files that are all one second or less.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(yes_waveform.numpy(), rate=yes_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(no_waveform.numpy(), rate=no_sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
