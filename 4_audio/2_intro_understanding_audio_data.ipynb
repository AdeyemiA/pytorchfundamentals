{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Audio Data\n",
    "\n",
    "Before we jump into build out our model, we need to first get a better understanding of our audio data. Here we will look at some key concepts and features of audio data, how we can visualize and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cassieb\\Anaconda3\\envs\\pytorchaudio\\lib\\site-packages\\torchaudio\\extension\\extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "# import the packages\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the audio backend?\n",
    "\n",
    "The audio backend provide I/O functions to work with and play audio files. If you are running this notebook on Windows we need to switch the audio backend to `soundfile`. The default `sox` audio backend is not support on windows. If you are running on Linux or Mac you can skip this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soundfile'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchaudio.set_audio_backend('soundfile')\n",
    "str(torchaudio.get_audio_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Dataset\n",
    "\n",
    "First lets download our dataset from PyTorch before we jump into the key concepts and terms to help us understand and work with audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speech commands dataset has different words - we are going to grab yes and no\n",
    "trainset_speechcommands = torchaudio.datasets.SPEECHCOMMANDS('./data', download=True)\n",
    "#figure out how to parse out classes in data loader? except folders?\n",
    "train_speechcommands_loader = torch.utils.data.DataLoader(trainset_speechcommands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Concepts, Transforms and Visualizations\n",
    "\n",
    "## Sample Rate\n",
    "\n",
    "First lets think about the digital representation of analog sound. How does sound get recorded anyway?! Just like with images we need to take our physical world and convert it to numbers or a digital represnation for a computer to understand. For audio a microphone is used to capture the sound and then its converted from analog sound to ditial sound by sampling at consitent intervals of time. This is called the `sample rate`. The higher the `sample rate` the higher the quality of the sound. The average sound sample rate is 48 kHz or 48,000 samples per second. This dataset was sampled at 16kHz so our sample rate is 16,000.\n",
    "\n",
    "With any machine learning dataset we want to make it as small as possible while not loosing the accuracy of our model. We are going to keep this sample rate however if you could play around with reducing the sampel rate of the audio to make the model smaller and see if it effects the quality of the model.\n",
    "\n",
    "## WAVE file\n",
    "\n",
    "You move likely have used a wave file before and undrestand that this is the format in which we save our digital representation of our analog audio to be shared and played. The Speech Commands dataset that we are using for this tutorial is stored in wave files that are all one second or less.\n",
    "\n",
    "## Waveform\n",
    "\n",
    "## Spectrogram\n",
    "\n",
    "## Mel Spectrogram\n",
    "\n",
    "## Mfcc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
