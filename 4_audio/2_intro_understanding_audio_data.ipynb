{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Audio Data\n",
    "\n",
    "Before we jump into build out our model, we need to first get a better understanding of our audio data. Here we will look at some key concepts and features of audio data, how we can visualize and transform the data.\n",
    "\n",
    "# torchaudio\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cassieb\\Anaconda3\\envs\\pytorchaudio\\lib\\site-packages\\torchaudio\\extension\\extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "# import the packages\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the audio backend?\n",
    "\n",
    "The audio backend provide I/O functions to work with and play audio files. If you are running this notebook on Windows we need to switch the audio backend to `soundfile`. The default `sox` audio backend is not support on windows. If you are running on Linux or Mac you can skip this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soundfile'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchaudio.set_audio_backend('soundfile')\n",
    "str(torchaudio.get_audio_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Create Dataset\n",
    "\n",
    "First lets download and parse out our yes/no dataset from the PyTorch Speech Commands dataset before we jump into the key concepts and terms to help us understand and work with audio data.\n",
    "\n",
    "After the dataset is download we will visualze all the classes available in the dataset and loop through to create the yes and no collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speech commands dataset has different words - we are going to grab yes and no\n",
    "trainset_speechcommands = torchaudio.datasets.SPEECHCOMMANDS('./data', download=True)\n",
    "#figure out how to parse out classes in data loader? except folders?\n",
    "train_speechcommands_loader = torch.utils.data.DataLoader(trainset_speechcommands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\code\\pytorchfundamentals\\4_audio\n",
      "['backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow', 'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero', '_background_noise_']\n"
     ]
    }
   ],
   "source": [
    "# get current directory and save as default\n",
    "default_dir = os.getcwd() \n",
    "print(default_dir)\n",
    "\n",
    "os.chdir('./data/SpeechCommands/speech_commands_v0.02/')\n",
    "labels = [name for name in os.listdir('.') if os.path.isdir(name)]\n",
    "# back to default directory\n",
    "os.chdir(default_dir)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove this and just visualize from the main downloaded set then in the next section\n",
    "# when the images are created loop thru and only create images for the yes no labels? Need to figure out how to \n",
    "# remove this processing time here\n",
    "\n",
    "trainset_speechcommands_no = [];\n",
    "trainset_speechcommands_yes = [];\n",
    "\n",
    "# split out yes and no data for training\n",
    "for i, data in enumerate(trainset_speechcommands):\n",
    "    if data[2] == 'yes': \n",
    "        trainset_speechcommands_yes.append(data)\n",
    "    elif data[2] == 'no':\n",
    "        trainset_speechcommands_no.append(data)\n",
    "        \n",
    "print(len(trainset_speechcommands_yes))\n",
    "print(len(trainset_speechcommands_no))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Concepts, Transforms and Visualizations\n",
    "\n",
    "Now that we have our dataset downloaded and yes/no labels parsed out. Lets think learn a little more about audio data and concepts by visualization and transform this dataset.\n",
    "\n",
    "## Sample Rate\n",
    "\n",
    "First lets think about the digital representation of analog sound. How does sound get recorded anyway?! Just like with images we need to take our physical world and convert it to numbers or a digital represnation for a computer to understand. For audio a microphone is used to capture the sound and then its converted from analog sound to ditial sound by sampling at consitent intervals of time. This is called the `sample rate`. The higher the `sample rate` the higher the quality of the sound. The average sound sample rate is 48 kHz or 48,000 samples per second. This dataset was sampled at 16kHz so our sample rate is 16,000.\n",
    "\n",
    "With any machine learning dataset we want to make it as small as possible while not loosing the accuracy of our model. We are going to keep this sample rate however if you could play around with reducing the sampel rate of the audio to make the model smaller and see if it effects the quality of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Waveform: {trainset_speechcommands_yes[0][0]}')\n",
    "print(f'Sample Rate: {trainset_speechcommands_yes[0][1]}')\n",
    "print(f'Label: {trainset_speechcommands_yes[0][2]}')\n",
    "print(f'ID: {trainset_speechcommands_yes[0][3]}')\n",
    "#print(f'Something: {trainset_speechcommands_yes[0][4]}')\n",
    "\n",
    "print(f'Waveform: {trainset_speechcommands_no[0][0]}')\n",
    "print(f'Sample Rate: {trainset_speechcommands_no[0][1]}')\n",
    "print(f'Label: {trainset_speechcommands_no[0][2]}')\n",
    "print(f'ID: {trainset_speechcommands_no[0][3]}')\n",
    "#print(f'Something: {trainset_speechcommands_no[0][4]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAVE file\n",
    "\n",
    "You move likely have used a wave file before and undrestand that this is the format in which we save our digital representation of our analog audio to be shared and played. The Speech Commands dataset that we are using for this tutorial is stored in wave files that are all one second or less.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(waveform.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waveform\n",
    "\n",
    "## Spectrogram\n",
    "\n",
    "## Mel Spectrogram\n",
    "\n",
    "## Mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
